 corpus based semantic kernel text classification using meaning values terms abstract text categorization plays crucial role academic commercial platforms due growing demand automatic organization documents kernel based classification algorithms support vector machines svm become highly popular task text mining mainly due relatively high classification accuracy several application domains well ability handle high dimensional sparse data prohibitive characteristics textual data representation recently increased interest exploitation background knowledge ontologies corpus based statistical knowledge text categorization shown replacing standard kernel functions linear kernel customized kernel functions take advantage background knowledge possible increase performance svm text classification domain based propose novel semantic smoothing kernel svm suggested approach based meaning measure calculates meaningfulness terms context classes documents vectors smoothed based meaning values terms context classes since efficiently make use class information smoothing process considered supervised smoothing kernel meaning measure based helmholtz principle gestalt theory previously applied several text mining applications document summarization feature extraction however best knowledge first study use meaning measure supervised setting build semantic kernel svm evaluated proposed approach conducting large number experiments well known textual datasets present results respect different experimental conditions compare results traditional kernels used svm linear kernel well several corpus based semantic kernels results show classification performance proposed approach outperforms kernels keywords support vector machinestext classificationsemantic kernelmeaninghigher order relations 1 introduction text categorization plays significantly important role recent years rapid growth textual information web especially social networks blogs forums enormous data increases contribution millions people every day automatically processing increasing amounts textual data important problem text classification defined automatically organizing documents predetermined categories several text categorization algorithms depend distance similarity measures compare pairs text documents reason similarity measures play critical role document classification apart structured data types textual data includes semantic information e sense conveyed words documents therefore classification algorithms utilize semantic information order achieve better results domain text classification documents typically represented terms words similar tokens frequencies representation approach one common one called bag words bow feature representation representation term constitutes dimension vector space independent terms document salton yang 1973 bow approach simple commonly used yet number restrictions main limitation assumes independency terms since documents bow model represented terms ignoring position document semantic syntactic connections words therefore clearly turns blind eye multi word expressions breaking apart furthermore treats polysemous words e words multiple meanings single entity instance term organ may sense body part appears context related biological structure sense musical instrument appears context refers music additionally maps synonymous words different components mentioned wang domeniconi 2008 principle steinbach et al 2000 analyze argue class two types vocabulary one core vocabulary closely related subject class type general vocabulary may similar distributions different classes two documents different classes may share many general words considered similar bow representation order address problems several methods proposed use measure relatedness term word sense disambiguation wsd text classification information retrieval domains semantic relatedness computations fundamentally categorized three knowledge based systems statistical approaches hybrid methods combine ontology based statistical information nasir et al 2013 knowledge based systems use thesaurus ontology enhance representation terms taking advantage semantic relatedness among terms examples see bloehdorn et al 2006 budanitsky hirst 2006 lee et al 1993 luo et al 2011 nasir et al 2013 scott matwin 1998 siolas alché buc 2000 wang domeniconi 2008 instance bloehdorn et al 2006 siolas alché buc 2000 distance words wordnet miller et al 1993 used capture semantic similarity english words study bloehdorn et al 2006 uses super concept declaration different distance measures words wordnet inverted path length ipl wu palmer measure resnik measure lin measure recent study kind found zhang 2013 uses hownet chinese semantic knowledge base second type semantic relatedness computations terms corpus based systems statistical analysis based relations terms set training documents performed order reveal latent similarities zhang et al 2012 one famous corpus based systems latent semantics analysis lsa deerwester et al 1990 partially solves synonymy problem finally approaches last category called hybrid since combine information acquired ontology statistical analysis corpus nasir et al 2013 altýnel et al 2014a recent survey zhang et al 2012 studies previous studies proposed several corpus based semantic kernels higher order semantic kernel hosk altýnel et al 2013 iterative higher order semantic kernel ihosk altýnel et al 2014a higher order term kernel hotk altýnel et al 2014b svm studies showed significant improvements classification performance traditional kernels svm linear kernel polynomial kernel rbf kernel taking advantage higher order relations terms documents instance hosk based higher order relations documents ihosk similar hosk since propose semantic kernel svm using higher order relations however ihosk makes use higher order paths documents terms iteratively therefore although performance ihosk superior complexity significantly higher higher order kernels simplified model hotk uses higher order paths terms sense similar previously proposed term based higher order learning algorithms higher order naïve bayes honb ganiz et al 2009 higher order smoothing hos poyraz et al 2012 poyraz et al 2014 article propose novel approach building semantic kernel svm name class meaning kernel cmk suggested approach smoothes terms document bow representation document vector represented term frequencies class based meaning values terms turn increases importance significant words meaningful terms class reducing importance general terms useful discriminating classes approach reduces mentioned disadvantages bow improves prediction abilities comparison standard linear kernels increasing importance class specific concepts synonymous closely related context class main novelty approach use class specific information smoothing process semantic kernel meaning values terms calculated according helmholtz principle gestalt theory balinsky et al 2010 balinsky et al 2011a balinsky et al 2011b balinsky et al 2011c context classes conducted several experiments various document datasets several different evaluation parameters especially terms training set amount experimental results show cmk widely outperforms performance kernels linear kernel polynomial kernel rbf kernel please note svm linear kernel accepted one best performing algorithms text classification virtually become de facto standard domain linear kernel inner product two document vectors used kernel function includes information terms documents share approach considered first order method since context scope consists single document however cmk make use meaning values terms classes case semantic relation two terms composed corresponding class based meaning values terms classes two terms important terms class resulting semantic relatedness value higher contrast semantic kernels make use wordnet wikipedia1 unsupervised fashion cmk directly incorporates class information semantic kernel therefore considered supervised semantic kernel one important advantages proposed approach relatively low complexity cmk less complex flexible approach background knowledge based approaches since cmk require processing large external knowledge base wikipedia wordnet furthermore since cmk constructed corpus based statistics always date similarly coverage problem semantic relations terms specific domain corpus leads another advantage cmk easily combined background knowledge based systems using wikipedia wordnet result cmk outperforms similar approaches cases terms accuracy execution time seen experimental results remainder paper organized follows background information related work including svm semantic kernels meaningfulness calculation summarized section 2 section 3 presents analyzes proposed kernel text classification algorithm experimental setup described section 4 corresponding experiment results including discussion points given section 5 finally conclude paper section 6 provide discussion probable future extension points current work 2 1 support vector machines classification problem support vector machines svm first proposed boser et al 1992 detailed analysis given vapnik 1995 general svm linear classifier aims finds optimal separating hyperplane two classes common representation linearly separable space w weight vector b bias document vector classified problem finding optimal separating hyperplane solved linearly constrained quadratic programming defined following equations problem eq 2 solved using lagrange method alpaydýn 2004 solution resultant decision function formulated lagrange multiplier k proper kernel function samples called support vectors important property kernel function satisfy mercer condition means semi positive alpaydýn 2004 consider kernel function kind similarity function calculates similarity values data points documents case transformed space therefore defining appropriate kernel direct effect finding better representation data points mentioned kontostathis pottenger 2006 siolas alché buc 2000 wang domeniconi 2008 popular kernel functions include linear kernel polynomial kernel rbf kernel problems multiclass classification two classes decomposition methodology used divide sub problems basically two categories multiclass methodology hsu lin 2002 one approach considers data one optimization formula wang et al 2014 whereas second approach based decomposing original problem several smaller binary problems solving separately combining solutions two widely used basic strategies category one rest one one approaches dumais et al 1998 hsu lin 2002 possible common use kernel function svm map transform data higher dimensional feature space impossible difficult find separating hyperplane classes original space besides svm work well high dimensional sparse data joachims 1998 benefits svm linear kernel one best performing algorithms text classification domain since textual data representation bow approach indeed quite sparse requires high dimensionality 2 2 semantic kernels text classification linear kernel widely used text classification domain since simplest kernel function represented eq 4 calculated kernel values depend inner products feature vectors documents mapping input space feature space done inner product linear kernel captures similarity documents much words share problem since considering semantic relations terms addressed incorporating semantic information words using semantic kernels described altýnel et al 2013 altýnel et al 2014a altýnel et al 2014b bloehdorn et al 2006 kandola et al 2004 luo et al 2011 nasir et al 2011 siolas alché buc 2000 tsatsaronis et al 2010 wang domeniconi 2008 wang et al 2014 according definition mentioned alpaydýn 2004 bloehdorn et al 2006 boser et al 1992 luo et al 2011 wang domeniconi 2008 function following form eq 7 valid kernel function eqs 7 d1 d2 input space vectors suitable mapping input space feature space siolas alché buc 2000 authors present semantic kernel intuitively based semantic relations english words wordnet popular widely used network semantic connections words connections hierarchies used measure similarities words authors use distance words wordnet hierarchical tree structure calculate semantic relatedness two words take advantage information enrich gaussian kernel results show using measured semantic similarities smoothing metric increases classification accuracy svm approach ignores multi word concepts treating single terms study bloehdorn et al 2006 uses super concept declaration semantic kernels aim create kernel algorithm captures knowledge topology belongs super concept expansion utilize mapping help semantic smoothing matrix q composed p pt contains super concept information corpus suggested kernel function given eq 8 results demonstrate get notable improvement performance especially situations feature representations highly sparse little training data exists bloehdorn et al 2006  production quality fish fingers different fish species production fish fingers achieved using fish species sardine sardina pilchardus walbaum 1792 whiting merlangius merlangus linnaeus 1758 pike perch sander lucioperca linnaeus 1758 quality changes battered fish patties period 8 months 18 c investigated according results microbiological chemical analysis fish fingers found within acceptable limits frozen storage 8 months however sensory analysis showed end frozen storage fish fingers made sardine could consumed rancidity recent years increase civilization socioeconomic factors like increasing numbers working women population led direct consumer preference ready eat foods foods cakes crackers burgers fish fingers marinated products etc made fish seafood products mostly preferred consumers around world various studies production quality stability ready eat foods done 1 2 3 4 5 6 7 reported whiting merlangius merlangus used fish finger production raw material german fish industry 80 years 8 frozen breaded fish portions fish fingers important fish products germany two kinds frozen fish fillets single frozen double frozen fillets used raw materials product quality differences fish portions made single frozen double frozen fish fillets gadus morhua theragra chalcogramma studied schubring 9 quality differences investigated sensory chemical physical analysis methods end study significant differences found sensory properties odor flavor texture overall acceptability products made single frozen double frozen fish fillets schubring 10 also studied texture sensory evaluation fish fingers made 13 different fish species turkey studies ready eat seafoods developing since seafoods mostly consumed fresh previous study fish finger production done turkey aim study produce fish fingers sardine abundance turkey whiting pike perch suitability producing fish fingers investigate quality changes fish fingers period 8 months storage 18 c fillets sardine whiting pike perch frozen 40 c stored 18 c 5 months study used raw materials whiting sardine caught may 2002 aegean region turkey izmir pike perch caught year eðridir lake turkey salt sugar carbonate red pepper black pepper curry cumin thyme garlic powder onion powder breadstick crumbs soybean oil used ingredients adhesive batter predust 18405506 yellow crumb 18614115 patented products used pasting coating materials respectively fish fingers produced contained 20 additive substances 80 minced fish additive substances consisted breadstick crumbs sugar soybean oil 16 spices 4 figure 1 shows flow sheet fish finger production fish finger production line koppens netherlands private food catering joint stock company izmir turkey used frozen fish fillets thawed overnight 4 c starting fish finger production moisture 11 crude fat 12 crude protein 13 ash 13 carbohydrate 14 cholesterol 15 analyses performed determine chemical compositions fish fish fingers order investigate chemical quality changes fish fingers frozen storage dimethylamine nitrogen dma n trimethylamine nitrogen tma n trimethylamine n oxide nitrogen tmao n values determined gas chromatographical method 16 total aerobic plate count apc coliform bacteria cb fecal cb fcb e coli counts coagulase positive staphylococcus aureus count yeast mold count salmonella determination made microbiological analysis fish fish fingers 17 18 scoring test used determination sensory quality fish fingers fish fingers fried teflon pan 5 min served panelists evaluate sensory attributes appearance texture odor flavor samples using five point descriptive scale 19 overall quality evaluation 20 excellent 18 2 19 9 good 15 2 18 1 good 11 2 15 1 medium 7 2 11 1 acceptability limit consumption 4 0 7 1 consumption limits 0 must destroyed chemical composition chemical compositions fish fish fingers presented table 1 moisture ratio fish finger groups decreased comparison raw materials protein ratios change significantly raw materials turning fish finger forms carbohydrate ratio fish finger groups increased presence breadstick crumbs pasting coating materials fish finger formulation cholesterol level fish fingers decreased certain level compared raw materials reported cholesterol level whiting 294 92 mg 100 g decreased 231 05 mg 100 g frying 20 moisture ratio fat ratio frozen squid stick found 43 74 17 64 respectively llorka et al 21 authors also stated fat ratio increased moisture decreased processing squid stick also determined alaska whiting theragra chalcogramma authors fat ash crude protein moisture 0 9 1 1 1 44 18 32 81 11 respectively results parallel values fish species fish fingers used study chemical quality changes sardine fish fingers dma n tma n values sardine fish fingers stayed low almost constant levels 8 months storage period tmao n value decreased 3 17 2 51 mg 100 g end storage period reported ammonia extracted sardine cod muscles increased rapidly together spoilage whereas dma n value stayed constant level 22 whiting fish fingers dma n value whiting fish fingers 1 25 mg 100 g beginning storage reached 5 50 mg 100 g end eighth month tmao n value whiting fish fingers determined 41 60 mg 100 g level decreased storage period karl et al 23 reported tma n tmao dma n values thawed fillets alaska whiting 0 5 0 7 79 99 4 6 mg n 100 g respectively values higher values fresh whiting pike perch fish fingers dma n value pike perch fish fingers determined 1 18 mg 100 g beginning storage increased level 3 69 mg 100 g end eighth month tmao n value pike perch fish fingers might tend decrease storage period value found 6 51 mg 100 g end storage volatile amines like ammonia tma n tvb n used indicators freshness spoilage concentrations amines increase time matter fact tmao n would excellent spoilage indicator cod also tmao n behavior different dma n decreased storage period 24 microbiological quality changes logapcs sardine whiting pike perch fillets around 4 6 g table 3 levels exceed maximum limits 7 logapc g microbiological criteria fresh frozen fish given international commission microbiological specifications foods 25 however observed sardine poor quality compared fish high apc high cb count presence e coli microbial load decreased fish finger production logapcs fish fingers stayed around 3 5 g frozen storage period maximum logapc breaded precooked fish products fish sticks fish cakes given 7 g 25 study apcs fish fingers lower maximum limits storage period 18 c 8 months fcb e coli detected fish fingers whiting pike perch addition cb fish fingers pike perch storage fcb detected fourth sixth months fish fingers sardine levels lower maximum limits 25 salmonella aureus yeast mold detected either fish fillets fish fingers storage period sensory quality changes sensory quality changes overall acceptability fish fish fingers given table 4 figs 2 3 4 5 respectively according sensory qualities storage period 8 months appearance texture odor flavor fish fingers started lowest values third month according results overall sensory evaluation fish sardine medium quality others good quality sensory properties fish fingers evaluated panelists perceived fishy taste sardine fish fingers characteristic taste could perceived fish fingers whiting pike perch sardine fish fingers generally received lowest points panelists storage period overall acceptable value sardine fish fingers decreased 8 60 lowest value among fish finger groups important thing rejection sardine fish fingers end eighth month storage period 18 c formation rancidity may suggested fresh material replacing frozen material could suitable production sardine fish fingers delay oxidation problems caused fat oxidation occurred slowly whiting pike perch compared sardine since white flesh result found fish fingers whiting pike perch could consumed end eighth month 18 c  new hybrid semi supervised algorithm text classification class based semantics abstract vector space models vsm commonly used language processing represent certain aspects natural language semantics semantics vsm comes distributional hypothesis states words occur similar contexts usually similar meanings previous work proposed novel semantic smoothing kernels based classspecific transformations kernels use class term matrices considered new type vsm using class context methods extract class specific semantics making use word distributions documents different classes study adapt two semantic classification approaches build novel high performance semi supervised text classification algorithm approaches include helmholtz principle based calculation term meanings context classes initial classification supervised term weighting based semantic kernel support vector machines svm final classification model approach used first phase especially good learning small datasets approach second phase specifically good eliminating noise relatively large noisy training sets building classification model overall semantic semi supervised learning algorithm approach effectively utilize abundant source unlabeled instances improve classification accuracy significantly especially amount labeled instances limited keywords semanticssemi supervised classificationtext classificationsemantic smoothing kernelclass based transformations 1 introduction vector space models vsm commonly used language processing represent aspects natural language semantics model documents simply represented points vector space closeness two points proportional semantic similarity several categories vsm including word context pair pattern term document matrices 5 pointed 5 semantics vsm comes distributional hypothesis states words occur similar contexts usually similar meanings 1 one main motivations use class documents context previous work proposed novel semantic smoothing kernels based class specific transformations 2 3 since kernels use class labels explicitly strictly supervised furthermore considered new type vsm consist term class matrices specific class labeled documents using class context matrices extract class specific semantics making use word distributions documents different classes semantic kernels integrated supervised classifiers e svm text classification could able outperform baseline classifiers using document based traditional vsm study combining one supervised semantic kernel based classification algorithm 3 class meanings based methodology classify unlabeled documents inspired 4 propose novel high performance semi supervised text classification algorithm machine learning applications two conventional strategies supervised learning unsupervised learning traditional supervised learning algorithms need set sufficient labeled data training set build classification model used predict class memberships unlabeled examples hand unsupervised learning solely based unlabeled samples need labeled data learn model train classifier attempts discover indirect structure unlabeled data 6 massive amounts accumulated data web particularly blogs forums social networks continue increase day day without doubt unfortunately available data pre assigned labels limit use several practical machine learning application fields text classification sentiment recognition speech recognition moreover generally time consuming tedious expensive assign labels manually importantly learning classifier labeled training data may generate sufficient performance situations labeled data inadequate many algorithms suggested exploiting utilizing unlabeled data support learning process better classification ssl approaches utilize labeled data also unlabeled data increase classification accuracy recently ssl become popular gained increased attention academic commercial platforms new machine learning strategy ssl different two ordinary classification approaches usage unlabeled data mitigate effect insufficient labeled data classifier accuracy many ssl systems offered past years like co training 7 self training 8 9 graph based methods 10 semi supervised support vector machines 6 em generative mixture models 11 transductive support vector machines 12 classification texts requires special techniques transform unstructured text structured format required classification algorithms usually vector features domain documents usually symbolized terms corresponding frequencies kind representation methodology actually popular one literature named bag words bow feature demonstration bow known simplest feature representation method term comprises dimension vector space independent terms document 13 bag mathematically similar set difference duplicate values sets order words lost bag representation similarly bag represented vector group bags also represented matrix rows documents columns term frequencies also called vector space model vsm although vsm bow approach simple commonly used several limitations discussed 2 one restrictions assumption independency terms documents represented term frequencies disregarding position document semantic syntactic links words big problem since clearly ignores multi word expressions separating moreover cannot handle polysemous words e words multiple meanings since treats single entity furthermore maps synonymous words different components discussed 14 principle steinbach et al 15 mention class two kinds vocabulary one core vocabulary closely correlated theme class type general vocabulary may similar distributions different classes consequently two documents different classes may commonly many general words categorized similar bow representation recent study 2 offer novel method constructing supervised semantic smoothing kernel svm name class meaning kernel cmk proposed method smoothens words document bow demonstration class based meaning values terms meaning scores words calculated based helmholtz principle gestalt theory 16 17 18 19 scope classes according experimental results cmk superior traditional kernels linear kernel polynomial kernel rbf kernel one previous studies suggest new classifier textual data named supervised meaning classifier smc 4 smc classifier uses meaning calculations based helmholtz principle gestalt theory smc meaningfulness terms scope classes calculated used classification document according experiment results new smc classifier outperforms multinomial naïve bayes mnb svm specifically insufficient training data another recent study 3 offer novel approach constructing supervised semantic kernel svm name class weighting kernel cwk proposed method smoothens document bow representation using class based weights words weights terms calculated based term weighting approach designed part feature extraction algorithm presented 20 21 according experimental results classification performance cwk higher classification performance commonly used kernels e linear kernel polynomial kernel rbf kernel inspired advantages cmk cwk smc motivated fact insufficient labeled data real life practical applications build new hybrid semi supervised form cwk smc called hybrid class semantics classifier hcsc precisely article suggest novel non iterative semi supervised methodology uses class based meaning values weights terms suggested approach utilizes labeled unlabeled data order build classifier first smoothens terms labeled documents bow demonstration document vector represented term frequencies class based meanings words way done cmk 2 smc 4 attempts find suitable labels unlabeled instances succeeded labeling process weighting terms unlabeled documents bow representation help meaning calculations 4 following hcsc combines original labeled data newly classified unlabeled data finally cwk applied enlarged labeled dataset order predict labels samples test dataset smoothing process hcsc increases significance important e meaningful words specific particular class reducing importance general terms similar distribution classes since method used transformation phase kernel function input space feature space considerably decreases effects mentioned disadvantages bow note hcsc advances accuracy svm compare linear kernel giving significance class specific concepts may possibly synonymous closely associated scope class hcsc uses semantic smoothing matrix transformation original space feature space semantic smoothing mechanism maps similar documents close positions feature space svm written using semantically nearby sets terms subject main novelty approach use meaning information labeling unlabeled data smoothing process semantic kernel meanings words calculated based helmholtz principle gestalt theory 16 17 18 19 scope classes documents 2 hcsc directly incorporates class information semantic kernel labeled instances additionally also incorporates unlabeled instances training process therefore considered semi supervised approach performed number experiments several document datasets numerous different labeled unlabeled test splits corpus according experimental results hcsc broadly outperforms performance baseline algorithms first gain proposed solution classification capability hcsc show performance difference robustness hcsc perform experiments various textual datasets different labeled unlabeled portions dataset experimental results demonstrate hcsc exceeds performance baselines including semi supervised form linear kernel linear kernel inner product two document vectors used kernel function exploits information shared term two documents approach categorized first order method since scope comprises single document though hcsc take advantage meaning values terms context classes way semantic relation two terms composed corresponding class based instance based meaning scores words classes consequently two words significant words class resulting semantic relatedness value greater second benefit offered approach relatively low complexity since hcsc need processing large external knowledge base like wordnet wikipedia besides hcsc built corpus based statistics always date result applied field without adjustments parameter optimizations improvement hcsc also forms foundation easily combined term based similarity measures also easy take advantage similarities terms derived semantic resource wikipedia wordnet according experimental results important improvement classification accuracy class dependent meaning values terms used svm compared performance baseline kernels best knowledge class dependent meaning values words used assign labels unlabeled instances utilized transformation phase svm first time bibliography also give important insights semantic smoothing words text classification rest paper organized follows first brief introduction supervised classification algorithms mainly svm semi supervised algorithms general text classification methods building class term matrix based vsms including meaningfulness calculations class context class based term weighting given section 2 section 3 explains analyzes proposed kernel text classification algorithm experimental setup corresponding experiment results discussion points presented section 4 section 5 finally conclude article section 6 provide discussion probable future extension points current work  music recommendation based adaptive feature user grouping introduction widespread use mp3 players cell phones availability music devices according user demands increased need accurate music information retrieval mir systems music recommendation one subtasks mir systems involves finding music suits personal taste 1 audioscrobbler irate musicstrands indiscover music recommendation systems today 2 usually music recommendation systems follow collaborative filtering content based approach collaborative filtering approach used amazon 3 new item rated users item recommended users based rating previous users 4 5 disadvantage collaborative approach new item arrives rated someone order used users content based approach based form distance items already rated user new item item recommended 2 6 7 8 order compute similarities music pieces different approaches suggested paper use low level musical features extracted audio signals past two studies 9 10 also considered collaborative content based methods music recommendation 9 bayesian network used include rating content data recommendation hybrid approach shown produce better recommendations using collaborative content based approach alone 10 also use hybrid approach evaluate cb content based col collaborative filtering sta statistical methods combinations base work 10 give information work hybris music recommendation system apart previous studies use observation person may base choice song certain aspects song rhythm melody consider audio features song according four different sets feratures mfcc mpitch beat stft obtained using marsyas software opihi cs uvic ca marsyas tzanetakis 14 use features used cluster songs user listened compact possible measure compactness use entropy criterion recommend certain number songs user certain time recommend certain percentage songs based content songs user listened far song user clustering user recommend remaining songs based popular songs time recommendation user past history unlike 10 instead system wide weights use adaptive weights user based user listening history addition algorithms introduced 17 paper introduce user group learning based algorithm rest paper organized follows sections 2 3 introduce data set used features extracted songs measure similarities section 4 contains descriptions recommendation systems recommendation success obtained using section 5 concludes paper  genetic approach data dimensionality reduction using special initial population abstract accurate classification data sets important phenomenon many applications multi dimensionality certain point contributes classification performance point incorporating attributes degrades quality classification pattern classification problem determining excluding least effective attribute performance classification likely improve task elimination least effective attributes pattern classification called data dimensionality reduction ddr ddr using genetic algorithms ddr ga aims discarding less useful dimensions organizing data set means genetic operators show wise selection initial population improves performance ddr ga considerably introduce method implement approach approach focuses using information obtained priori selection initial chromosomes work compares performance ga initiated randomly selected initial population performance ones initiated wisely selected one furthermore results indicate approach provides accurate results compared purely random one reasonable amount time keywords data dimensionality reduction feature extraction genetic algorithms attribute ranking attribute quality series series lecture notes computer science lncs including subseries lecture notes artificial intelligence lnai lecture notes bioinformatics lnbi established medium publication new developments computer science information technology research teaching quickly informally high level cornerstone lncs editorial policy unwavering commitment report latest results areas computer science information technology research development education lncs always enjoyed close cooperation computer science r community numerous renowned academics prestigious institutes learned societies mission serve community providing valuable publication service lncs commenced publication 1973 quite rapidly attracted attention least thus far unprecedented publication turnaround times 19 80s 1990s witnessed substantial growth series particularly terms volumes published late 1990s developed systematic approach providing lncs full text electronic version parallel printed books another new feature introduced late 1990s conceptualization couple color cover sublines still original research results reported proceedings postproceedings remain core lncs 